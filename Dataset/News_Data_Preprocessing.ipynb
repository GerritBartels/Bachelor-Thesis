{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preprocessing**"
      ],
      "metadata": {
        "id": "6wocMVtxgfv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "S4T7bSlGeXv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6lAbgeGeOKm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re \n",
        "import random\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm.auto import tqdm \n",
        "import contractions\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**\n",
        "\n",
        "The dataset containing crawled english news data from 2019 provided by the WMT21 conference can be found here: https://data.statmt.org/news-crawl/en/ \n"
      ],
      "metadata": {
        "id": "SUzLMljAe0LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If dataset is saved on google drive you can mount your cloud storage here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "oz8FaFJueas8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit this variable when your dataset is saved locally \n",
        "path = \"YourPathHere\""
      ],
      "metadata": {
        "id": "ezPRUdW-pkX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset:"
      ],
      "metadata": {
        "id": "_TtFA2hsjeYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = []\n",
        "\n",
        "with open(f\"{path}news_data.deduped\", encoding=\"utf-8\") as f:\n",
        "    with tqdm(total=33600797) as pbar:\n",
        "        for line in f:\n",
        "            news.append(line)\n",
        "            pbar.update(1)\n",
        "\n",
        "del(news[19195150])\n",
        "\n",
        "print(f\"Remaining sentences: {len(news)}\")"
      ],
      "metadata": {
        "id": "_a5lcUUThxCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expand all contractions and split sentences:"
      ],
      "metadata": {
        "id": "qTITRRh7jWVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sent = []\n",
        "for line in tqdm(news):\n",
        "    news_sent.extend(nltk.sent_tokenize(contractions.fix(line, slang=False)))\n",
        "\n",
        "print(f\"Remaining sentences: {len(news_sent)}\")"
      ],
      "metadata": {
        "id": "uRni6-WojWzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing using regex:\n"
      ],
      "metadata": {
        "id": "q88ZlBBIiefh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperlinks\n",
        "news_cleaned = [re.sub(r'[\\'’\"“”]|http\\S+|\\n', '', sent) for sent in tqdm(news_sent)]\n",
        "\n",
        "# multiple occurences of . - — or *\n",
        "news_cleaned = [re.sub(r'\\.\\.+|—+|-+|\\*\\*+', ' ',sent) for sent in tqdm(news_cleaned)]\n",
        "\n",
        "# digits\n",
        "news_cleaned = [re.sub(r'\\d+([\\.,-:]?\\d+)*', ' <NUM> ', sent) for sent in tqdm(news_cleaned)]\n",
        "\n",
        "# USA/US        \n",
        "news_cleaned = [re.sub(r'usa\\b|USA\\b|U\\.S\\.A\\.|u\\.s\\.a\\.', 'Platzhalter', sent) for sent in tqdm(news_cleaned)]\n",
        "news_cleaned = [re.sub(r'US\\b|U\\.S\\.|u\\.s\\.|U\\.s\\.', 'U.S ', sent) for sent in tqdm(news_cleaned)]\n",
        "news_cleaned = [re.sub(r'Platzhalter', 'U.S.A ', sent) for sent in tqdm(news_cleaned)]\n",
        "\n",
        "print(\"Example sentences: \")\n",
        "news_cleaned[5985:5990]"
      ],
      "metadata": {
        "id": "7Bxx-1DCidXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply nltk's word tokenizer to the lowercased sentences:"
      ],
      "metadata": {
        "id": "BZrKyb5JizEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_tokenized = [word_tokenize(sent.lower()) for sent in tqdm(news_cleaned)]\n",
        "\n",
        "print(f\"Remaining sentences: {len(news_tokenized)}\")"
      ],
      "metadata": {
        "id": "jQZyHDqniuxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermediate save to free up memory:"
      ],
      "metadata": {
        "id": "Pi0c5wXdkTiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{path}news_data_preprocessed_cache.csv\", \"w\", encoding='utf8', newline=\"\") as output_file:\n",
        "    writer = csv.writer(output_file)\n",
        "    writer.writerows(news_tokenized)"
      ],
      "metadata": {
        "id": "yju7hdbAkS8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the intermediate save and create a token frequency dict. Sentences shorter than 5 tokens are removed:"
      ],
      "metadata": {
        "id": "ylAbxPMQkpnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_tokenized = []\n",
        "freqs = {}\n",
        "\n",
        "with open(f\"{path}news_data_preprocessed_cache.csv\", encoding='utf-8', newline=\"\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    with tqdm(total=43989540) as pbar:\n",
        "        for sent in reader:\n",
        "            if len(sent) > 5:\n",
        "                news_tokenized.append(sent)\n",
        "                for word in sent:\n",
        "                    freqs[word] = freqs.get(word, 0) + 1\n",
        "            pbar.update(1)\n",
        "\n",
        "print(f\"Number of tokens: {len(freqs)}\")"
      ],
      "metadata": {
        "id": "Y84BZlDDko4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the freq dict: "
      ],
      "metadata": {
        "id": "5PGBcWWVlSBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'{path}freq_dict.csv', 'w', encoding='utf8', newline=\"\") as output_file:\n",
        "    w = csv.DictWriter(output_file, freqs.keys())\n",
        "    w.writeheader()\n",
        "    w.writerow(freqs)  "
      ],
      "metadata": {
        "id": "GPJ6HEwGlQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing sentences that contain words which occur less than 10000 times:"
      ],
      "metadata": {
        "id": "6gS1nPH1lfPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove=False\n",
        "cache = []\n",
        "\n",
        "for sent in tqdm(news_tokenized):\n",
        "    for word in sent:\n",
        "        if freqs[word]<=10000:\n",
        "            remove=True\n",
        "    if remove == False:\n",
        "        cache.append(sent)\n",
        "    remove=False \n",
        "\n",
        "\n",
        "vocab_set = set()\n",
        "for sent in cache:\n",
        "    vocab_set = vocab_set.union(set(sent))\n",
        "\n",
        "print(f\"Remaining sentences: {len(cache)}\")\n",
        "print(f\"Remaining vocab: {len(vocab_set)}\")"
      ],
      "metadata": {
        "id": "5a0vT00KleSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermediate save to free up memory:"
      ],
      "metadata": {
        "id": "n0a_LYFnoc_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{path}news_data_preprocessed_voc_6826.csv\", \"w\", encoding='utf8', newline=\"\") as output_file:\n",
        "    writer = csv.writer(output_file)\n",
        "    writer.writerows(cache)"
      ],
      "metadata": {
        "id": "S-WPKHpBoaAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the intermediate save:"
      ],
      "metadata": {
        "id": "8nViRU3Co1sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{path}news_data_preprocessed_voc_6826.csv\", encoding='utf-8', newline=\"\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    news_tokenized = list(reader)"
      ],
      "metadata": {
        "id": "HFb9dRz-ozy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set a seed to make results comparable.\n",
        "Shuffle the dataset once, to obtain random train and test partitions later:"
      ],
      "metadata": {
        "id": "y3gUbQ6hpHDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(69)\n",
        "\n",
        "random.shuffle(news_tokenized)\n",
        "\n",
        "news_tokenized = news_tokenized[:750000]"
      ],
      "metadata": {
        "id": "lL9Z7IJqpEPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final save:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xoccSpbmpVal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{path}news_data_preprocessed.csv\", \"w\", encoding='utf8', newline=\"\") as output_file:\n",
        "    writer = csv.writer(output_file)\n",
        "    writer.writerows(news_tokenized)"
      ],
      "metadata": {
        "id": "wY6xlBEmpUwa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}