{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "72fCcBS1okzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm \n",
        "from gensim.models import Word2Vec\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
        "\n",
        "# Adapt this variable to the path of the cloned repository\n",
        "path = \"YourPathHere\"\n",
        "\n",
        "# For importing custom modules\n",
        "import sys\n",
        "sys.path.append(f'{path}/Modules')"
      ],
      "metadata": {
        "id": "rMYTIuApohYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount google drive:"
      ],
      "metadata": {
        "id": "tNGvbSGioz2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "qYRtVrLmo17m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "ju5cCQLKo5hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import dataset_cleanup"
      ],
      "metadata": {
        "id": "MrEhDhr6zd2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = f\"{path}/Dataset/news_data_preprocessed.csv\"\n",
        "min_sent_len=10\n",
        "max_sent_len=28\n",
        "\n",
        "cleaned_data, max_seq_length = dataset_cleanup(data_path=data_path, \n",
        "                                               min_sent_len=min_sent_len, \n",
        "                                               max_sent_len=max_sent_len)\n",
        "\n",
        "train_data = []\n",
        "for sent in cleaned_data:\n",
        "    train_data.append(sent[1:])"
      ],
      "metadata": {
        "id": "EhRtV3r7zgiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load word2vec embeddings**"
      ],
      "metadata": {
        "id": "XV746SzFpG73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load previously saved embeddings\n",
        "word2vec_model = Word2Vec.load(f\"{path}/Skip-Gram Embeddings/skip-gram_embeddings.model\")\n",
        "\n",
        "print(\"Examine the trained embeddings: \")\n",
        "word2vec_model.most_similar(\"<NUM>\", topn=10)"
      ],
      "metadata": {
        "id": "aN46TsKxpGdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data used for evaluation:"
      ],
      "metadata": {
        "id": "BOcVbcnTpVhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2index_dict = {token: token_index for token_index, token in enumerate(word2vec_model.wv.index2word)}\n",
        "\n",
        "sent2index = []\n",
        "\n",
        "for sent in train_data:\n",
        "    sent = [word2index_dict[key] for key in sent]\n",
        "    sent2index.append(sent)"
      ],
      "metadata": {
        "id": "Yzo03re1pael"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_token = word2index_dict[\"<Start>\"]\n",
        "end_token = word2index_dict[\"<End>\"]\n",
        "print(f\"<Start>: {start_token}\")\n",
        "print(f\"<End>: {end_token}\")"
      ],
      "metadata": {
        "id": "Yo93mtveslue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "_LHaNubbpwAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the reference data used for Bleu, Self-Bleu, Word Frequency and Jenssen-Shannon Distance calculations:"
      ],
      "metadata": {
        "id": "XGb7TmPRp1Uq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ihQeKVsno7j"
      },
      "outputs": [],
      "source": [
        "reference_data = []\n",
        "for sent in sent2index[int(len(sent2index)*0.85):]:\n",
        "    temp = []\n",
        "    for token_id in sent:\n",
        "        if token_id == end_token:\n",
        "            break\n",
        "        temp.append(word2vec_model.wv.index2word[token_id])\n",
        "    reference_data.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avg Sentence Length:"
      ],
      "metadata": {
        "id": "6Iu4XyzZqVjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_length = 0.0\n",
        "for sent in reference_data[:10000]:\n",
        "    avg_length += len(sent)\n",
        "print(f\"Average length of the test sentences: {avg_length/10000} tokens\")"
      ],
      "metadata": {
        "id": "KYxPMJyzqYk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU-4:"
      ],
      "metadata": {
        "id": "8M6NomfDqs2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_grams = 4\n",
        "score_bleu = corpus_bleu([reference_data[:10000] for i in range(10000)], reference_data[-10000:], weights=tuple(1./n_grams for _ in range(n_grams)), smoothing_function=SmoothingFunction().method1)\n",
        "print(f\"BLEU-4 Score of the test sentences: {score_bleu}\")"
      ],
      "metadata": {
        "id": "7qHl4BYUrCG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self BLEU-4:"
      ],
      "metadata": {
        "id": "sr2K-bqTtxBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "references = []\n",
        "\n",
        "hyps = []\n",
        "\n",
        "for idx, hyp in enumerate(tqdm(reference_data[:10000])):\n",
        "    \n",
        "    bleu_reference = copy.deepcopy(reference_data[:10000])\n",
        "\n",
        "    bleu_reference.pop(idx)\n",
        "    \n",
        "    references.append(bleu_reference)\n",
        "    \n",
        "    hyps.append(hyp)\n",
        "    \n",
        "\n",
        "self_bleu = corpus_bleu(references, hyps, weights = tuple(1./n_grams for _ in range(n_grams)), smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "print(f\"Self BLEU-4 Score of the test sentences: {self_bleu}\")"
      ],
      "metadata": {
        "id": "p6zwVzRQM6o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JS Distance and word overlap of the top 12 words:"
      ],
      "metadata": {
        "id": "etSEOYWQt0ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_counts(ref, gen):\n",
        "\n",
        "    q_ref = dict.fromkeys(set(list(ref.keys())+list(gen.keys())))\n",
        "    k_gen = dict.fromkeys(set(list(ref.keys())+list(gen.keys())))\n",
        "\n",
        "    for key in tqdm(q_ref.keys()):\n",
        "        try: \n",
        "            q_ref[key] = ref[key]\n",
        "        except:\n",
        "            q_ref[key] = 0 \n",
        "        try:\n",
        "            k_gen[key] = gen[key]\n",
        "        except:\n",
        "            k_gen[key] = 0\n",
        "\n",
        "    return list(q_ref.values()), list(k_gen.values())\n",
        "\n",
        "\n",
        "ref_word_freq = {}\n",
        "gen_word_freq = {}\n",
        "\n",
        "ref_sent_length = {}\n",
        "gen_sent_length = {}\n",
        "\n",
        "jsd_sent_length = 0.0\n",
        "jsd_word_count = 0.0\n",
        "\n",
        "\n",
        "for idx, sample in enumerate(tqdm(reference_data[:10000])):\n",
        "    \n",
        "\n",
        "    # Get the sentence lengths\n",
        "    ref_length = len(reference_data[-10000+idx])\n",
        "    gen_length = len(sample)\n",
        "\n",
        "    # Increment the respective sentence length entry for ref and gen \n",
        "    if ref_length in ref_sent_length:\n",
        "            ref_sent_length[ref_length] += 1\n",
        "    else:\n",
        "        ref_sent_length[ref_length] = 1\n",
        "    \n",
        "    if gen_length in gen_sent_length:\n",
        "            gen_sent_length[gen_length] += 1\n",
        "    else:\n",
        "        gen_sent_length[gen_length] = 1\n",
        "\n",
        "\n",
        "    # Loop over the tokens and increment the word count for ref and gen \n",
        "    for token in reference_data[-10000+idx]:\n",
        "        \n",
        "        if token in ref_word_freq:\n",
        "            ref_word_freq[token] += 1\n",
        "        else:\n",
        "            ref_word_freq[token] = 1\n",
        "\n",
        "    for token in sample:\n",
        "        \n",
        "        if token in gen_word_freq:\n",
        "            gen_word_freq[token] += 1\n",
        "        else:\n",
        "            gen_word_freq[token] = 1\n",
        "\n",
        "# Calculate Jensen-Shannon Distance \n",
        "aligned_sent_lengths = align_counts(ref_sent_length, gen_sent_length)\n",
        "jsd_sent_length = jensenshannon(aligned_sent_lengths[0], aligned_sent_lengths[1], 2)\n",
        "\n",
        "aligned_word_counts = align_counts(ref_word_freq, gen_word_freq)\n",
        "jsd_word_count = jensenshannon(aligned_word_counts[0], aligned_word_counts[1], 2)\n",
        "\n",
        "print(f\"Jensen-Shannon distance for the sentence length frequencies: {jsd_sent_length}\")\n",
        "print(f\"Jensen-Shannon distance for the word counts: {jsd_word_count}\")\n",
        "print(f\"Top 12 words in the first 10000 reference sentences: {list(dict(sorted(ref_word_freq.items(), key=lambda item: item[1], reverse=True)).items())[:12]}\")\n",
        "print(f\"Top 12 words in the last 10000 reference sentences: {list(dict(sorted(gen_word_freq.items(), key=lambda item: item[1], reverse=True)).items())[:12]}\")"
      ],
      "metadata": {
        "id": "2aov7F6-uGW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}