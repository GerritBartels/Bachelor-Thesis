{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ],
      "metadata": {
        "id": "_2scBa0S3l37"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PY3TgYbmk7k"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt this variable to the path of the cloned repository\n",
        "path = \"YourPathHere\""
      ],
      "metadata": {
        "id": "bNXJEmuV1p4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the reference data and perform necessary preprocessing steps:"
      ],
      "metadata": {
        "id": "i7emw1ul2qe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open(f\"{path}/Dataset/news_data_preprocessed.csv\", encoding='utf-8', newline=\"\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    news_tokenized = list(reader)\n",
        "    \n",
        "# Replace <, NUM, > with <number>\n",
        "for idefix, sent in enumerate(news_tokenized):\n",
        "    for obelix, token in enumerate(sent):\n",
        "        if token==\"<\":\n",
        "            del sent[obelix:obelix+3]\n",
        "            news_tokenized[idefix].insert(obelix, \"<number>\")\n",
        "\n",
        "avg = 0.0\n",
        "count = 0\n",
        "# Delete sentences that are shorter than 10 and longer than 28 tokens\n",
        "news_cache= []\n",
        "for sent in news_tokenized:\n",
        "    if len(sent)>9 and len(sent)<29 and not (\"(\" in sent and \"hr\" in sent):\n",
        "        news_cache.append(sent)\n",
        "        avg += len(sent)\n",
        "    else:\n",
        "        count+=1\n",
        "\n",
        "news_tokenized = news_cache\n",
        "\n",
        "\n",
        "\n",
        "print(avg/len(news_cache))\n",
        "print(count)\n",
        "\n",
        "\n",
        "for sent in news_tokenized:\n",
        "    sent.insert(len(sent), \"</s>\")\n",
        "    sent.insert(0, \"<s>\")\n",
        "\n",
        "\n",
        "max_length = 0\n",
        "idx = 0\n",
        "for sent in news_tokenized:\n",
        "    if len(sent) > max_length:\n",
        "        max_length = len(sent)\n",
        "\n",
        "print(f\"Longest Sentence has {max_length} tokens.\")   \n",
        "print(len(news_tokenized))\n",
        "\n",
        "\n",
        "all_sents = []\n",
        "for sent in news_tokenized:\n",
        "    all_sents += sent\n",
        "\n",
        "\n",
        "all_sents_batched = []\n",
        "counter = 0\n",
        "append = False\n",
        "\n",
        "for idx, word in enumerate(all_sents):\n",
        "\n",
        "    counter += 1\n",
        "\n",
        "    if word == \"<s>\" and append == False:\n",
        "        append = True\n",
        "        all_sents_batched.append(all_sents[idx:idx+30])\n",
        "        counter = 1\n",
        "\n",
        "    elif counter == 30:\n",
        "        append = False\n",
        "\n",
        "all_sents_batched = all_sents_batched[:-1]\n",
        "\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for sent in all_sents_batched:\n",
        "    train_data.append(sent[1:])"
      ],
      "metadata": {
        "id": "LPWw4S88yyjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct reference data:"
      ],
      "metadata": {
        "id": "qOH9S8z72YpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_data = []\n",
        "for sent in train_data[int(len(train_data)*0.85):]:\n",
        "    temp = []\n",
        "    for token in sent:\n",
        "        if token == \"</s>\":\n",
        "            break\n",
        "        temp.append(token)\n",
        "    reference_data.append(temp)"
      ],
      "metadata": {
        "id": "rzUq4Mc12Xxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load generated sentences from a model:"
      ],
      "metadata": {
        "id": "BO4mXwzB2b6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_data = []\n",
        "\n",
        "# Chose one of the following: cVAELM_InferSent, LSTMLM_InferSent, GSGAN_InferSent, LaTextGAN_InferSent, GPT-2_Small_InferSent\n",
        "with open(f\"{path}/Evaluation/FID/cVAELM_InferSent.csv\", encoding='utf-8', newline=\"\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    gen_data = list(reader)\n",
        "\n",
        "# Change our tokens, such that they correspond to the tokens used in GloVe\n",
        "for sentence in gen_data:\n",
        "    for idx, word in enumerate(sentence):\n",
        "        if word == \"<NUM>\":\n",
        "            sentence[idx] = \"<number>\"\n",
        "        elif word == \"<End>\":\n",
        "            sentence[idx] = \"</s>\"\n",
        "        elif word == \"<Start>\":\n",
        "            sentence[idx] = \"<s>\""
      ],
      "metadata": {
        "id": "piJm_nkV5EoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download GloVe embeddings used in InferSent:"
      ],
      "metadata": {
        "id": "fnCZU8zO3XNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir GloVe\n",
        "!curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip GloVe/glove.840B.300d.zip -d GloVe/"
      ],
      "metadata": {
        "id": "yNUuyCv3pCwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir encoder\n",
        "!curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl"
      ],
      "metadata": {
        "id": "zPUbpH5uoIlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the InferSent model:"
      ],
      "metadata": {
        "id": "jUp9kq_T3d4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(f'{path}/Evaluation/FID')\n",
        "\n",
        "from InferSent_models import InferSent\n",
        "model_version = 1\n",
        "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
        "model = InferSent(params_model)\n",
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "metadata": {
        "id": "P2b1uIVZnhc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
        "W2V_PATH = 'GloVe/glove.840B.300d.txt' if model_version == 1 else 'fastText/crawl-300d-2M.vec'\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "metadata": {
        "id": "1voL7UKuqqc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(reference_data[:10000]+gen_data, tokenize=False)\n",
        "#model.build_vocab(reference_data[:10000]+reference_data[-10000:], tokenize=False)"
      ],
      "metadata": {
        "id": "45OggrKK4Qw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep it on CPU or put it on GPU\n",
        "use_cuda = True\n",
        "model = model.cuda() if use_cuda else model"
      ],
      "metadata": {
        "id": "QTtUit0h3MmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_embeddings = model.encode(reference_data[:10000], bsize=128, tokenize=False, verbose=True)\n",
        "print('nb sentences encoded : {0}'.format(len(test_embeddings)))\n",
        "\n",
        "gen_embeddings = model.encode(gen_data, bsize=128, tokenize=False, verbose=True)\n",
        "print('nb sentences encoded : {0}'.format(len(gen_embeddings)))"
      ],
      "metadata": {
        "id": "Fkk7jz2QrMY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To calculate InferSent on the train data itself\n",
        "# gen_embeddings = model.encode(reference_data[-10000:], bsize=128, tokenize=False, verbose=True)\n",
        "# print('nb sentences encoded : {0}'.format(len(gen_embeddings)))"
      ],
      "metadata": {
        "id": "ARyp4JLzYeb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate frechet inception distance\n",
        "def calculate_fid(test_embeddings, gen_embeddings):\n",
        "\n",
        "    # Calculate mean and covariance statistics\n",
        "    mu1, sigma1 = np.mean(test_embeddings, axis=0), np.cov(test_embeddings, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(gen_embeddings, axis=0), np.cov(gen_embeddings, rowvar=False)\n",
        "\n",
        "    # Calculate sum squared difference between means\n",
        "    diff = np.sum((mu1 - mu2)**2.0)\n",
        "\n",
        "    # Calculate sqrt of product between cov\n",
        "    square_root = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "    # Check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(square_root):\n",
        "      square_root = square_root.real\n",
        "\n",
        "    # Calculate score\n",
        "    frechet_infersent_dist = diff + np.trace(sigma1 + sigma2 - 2.0 * square_root)\n",
        "\n",
        "    return round(frechet_infersent_dist, 4)"
      ],
      "metadata": {
        "id": "rBghw3hAs7xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_fid(test_embeddings, gen_embeddings)"
      ],
      "metadata": {
        "id": "8hxV8IulHRA4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}