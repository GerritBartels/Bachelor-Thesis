{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4THDNRpTFYN"
      },
      "source": [
        "# **GPT 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL_RrYE1TFYP"
      },
      "source": [
        "The following notebook is inspired by and uses parts of these two huggingface notbooks:\n",
        "\n",
        "* [Train your tokenizer from scratch](https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb)\n",
        "* [Train your language model from scratch](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling_from_scratch-tf.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCPYzsuTTFYQ"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LSnDTDU9QqQ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from tqdm.auto import tqdm \n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import GPT2TokenizerFast\n",
        "from transformers import DefaultDataCollator\n",
        "from transformers import AutoConfig, TFAutoModelForCausalLM\n",
        "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, AddedToken\n",
        "\n",
        "\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot') \n",
        "\n",
        "# Adapt this variable to the path of the cloned repository\n",
        "path = \"YourPathHere\"\n",
        "\n",
        "# For importing custom modules\n",
        "import sys\n",
        "sys.path.append(f'{path}/Modules')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enck8mBjTFYT"
      },
      "source": [
        "Mount Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ67u9EbTFYU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_j1w_MtTFYV"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import dataset_cleanup"
      ],
      "metadata": {
        "id": "xza0Ftk_lI-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = f\"{path}/Dataset/news_data_preprocessed.csv\"\n",
        "min_sent_len=10\n",
        "max_sent_len=28\n",
        "\n",
        "cleaned_data, max_seq_length = dataset_cleanup(data_path=data_path, \n",
        "                                                    min_sent_len=min_sent_len, \n",
        "                                                    max_sent_len=max_sent_len)"
      ],
      "metadata": {
        "id": "EqqOEUKZlJYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGaD3_jSTFYY"
      },
      "source": [
        "## **Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEuSsUMOTFYY"
      },
      "outputs": [],
      "source": [
        "vocab_size=6803\n",
        "\n",
        "tokenizer = Tokenizer(models.WordLevel())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit(add_prefix_space=True)\n",
        "trainer = trainers.WordLevelTrainer(vocab_size=vocab_size)\n",
        "tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, add_prefix_space=True)\n",
        "tokenizer.add_special_tokens({\"eos_token\": \"<End>\", \"bos_token\": \"<Start>\"})\n",
        "tokenizer = tokenizer.train_new_from_iterator(cleaned_data, vocab_size=vocab_size, is_split_into_words=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1kWQo2TFYZ"
      },
      "source": [
        "## **Data Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-ZrSJTBTFYZ"
      },
      "source": [
        "Last cleanup and convert data to dicts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtoaNCLyTFYZ"
      },
      "outputs": [],
      "source": [
        "train_data = {'text': cleaned_data[:int(0.85*(len(cleaned_data)))]}\n",
        "del train_data[\"text\"][115814]\n",
        "del train_data[\"text\"][121909]\n",
        "\n",
        "validation_data = {\"text\": cleaned_data[int(0.85*(len(cleaned_data))):]}\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "\n",
        "validation_dataset = Dataset.from_dict(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bkCN2mLTFYa"
      },
      "source": [
        "Tokenize already tokenized data:\n",
        "\n",
        "(necessary step for huggingface transformer and doesn't alter the data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS2m25YM3l-z"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], is_split_into_words=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVAO0H8u3l-3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
        "\n",
        "tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q26ZzTp5TFYb"
      },
      "source": [
        "Add labels for data collator and convert to tf dataset:\n",
        "\n",
        "(They simply need to be a copy of the input ids, since the actual shifting is done at runtime by the collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaAJy5Hu3l_B"
      },
      "outputs": [],
      "source": [
        "def add_labels(result):\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXUSfBrq3l_C"
      },
      "outputs": [],
      "source": [
        "train_data = tokenized_train_dataset.map(\n",
        "    add_labels,\n",
        "    batched=True,\n",
        "    batch_size=256,\n",
        "    num_proc=1\n",
        ")\n",
        "\n",
        "\n",
        "validation_data = tokenized_validation_dataset.map(\n",
        "    add_labels,\n",
        "    batched=True,\n",
        "    batch_size=256,\n",
        "    num_proc=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAMK42IW9Qqh"
      },
      "outputs": [],
      "source": [
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "train_dataset = train_data.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "validation_dataset = validation_data.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCc6ZV0kTFYd"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0VfKjKJTFYd"
      },
      "source": [
        "Create randomly initialized GPT2-small model for language modelling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPqQA3TT3l_I"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer), bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "News_GPT2 = TFAutoModelForCausalLM.from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohM3VmT8TFYd"
      },
      "source": [
        "Model summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PWQaWXqYTG2"
      },
      "outputs": [],
      "source": [
        "News_GPT2(News_GPT2.dummy_inputs)  \n",
        "News_GPT2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1VHrSNNTFYe"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdNeN7IFTFYe"
      },
      "source": [
        "Set some hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbSwEhQ63l_L"
      },
      "outputs": [],
      "source": [
        "learning_rate = 2e-5\n",
        "weight_decay = 0.0\n",
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "News_GPT2.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXQYnY2STFYe"
      },
      "source": [
        "Calculate initial train and test losses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91G2lv7nTFYe"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_losses.append(News_GPT2.evaluate(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnbEEHTnTFYf"
      },
      "outputs": [],
      "source": [
        "test_losses = []\n",
        "test_losses.append(News_GPT2.evaluate(validation_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N8bnABmTFYf"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyZvu_MF3l_P"
      },
      "outputs": [],
      "source": [
        "News_GPT2_losses = News_GPT2.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO86Wkr_TFYg"
      },
      "source": [
        "Save the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQAc6UZ7TFYg"
      },
      "outputs": [],
      "source": [
        "News_GPT2.save_pretrained(\"GPT-2_Small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7UcECf2TFYg"
      },
      "source": [
        "Plot loss values for training and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_kphRO5TFYg"
      },
      "outputs": [],
      "source": [
        "for i in News_GPT2_losses.history['val_loss']:\n",
        "    test_losses.append(i)\n",
        "    \n",
        "for i in News_GPT2_losses.history['loss']:\n",
        "    train_losses.append(i)\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "fig1, ax1 = plt.subplots(nrows=1, ncols=1, figsize = (10, 6))\n",
        "ax1.plot(train_losses, label='training')\n",
        "ax1.plot(test_losses, label='test')\n",
        "ax1.set(ylabel='Loss', xlabel='Epochs', title=f'Average loss over 10 epochs')\n",
        "ax1.legend()\n",
        "plt.savefig(f\"news_gpt2_loss_plot10_transparent\", dpi=500.0, format=\"png\", transparent=True)\n",
        "plt.savefig(f\"news_gpt2_loss_plot10\", dpi=500.0, format=\"png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS8o1lF2TFYh"
      },
      "source": [
        "## **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii3trbeKTFYh"
      },
      "source": [
        "Import evaluation module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_dNuWYqTFYh"
      },
      "outputs": [],
      "source": [
        "import evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip1JfwzPTFYh"
      },
      "source": [
        "Load the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78_V-ZvuTFYh"
      },
      "outputs": [],
      "source": [
        "News_GPT2 = TFAutoModelForCausalLM.from_pretrained(\"GPT-2_Small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwE9O9hETFYh"
      },
      "source": [
        "Create csv containing sentences for InferSent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcfbR9rJTFYi"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "generator_truncated = pipeline(\"text-generation\", model=News_GPT2, tokenizer=tokenizer, return_full_text=False)\n",
        "sents = generator_truncated(\"<Start> \", max_length=max_seq_length, pad_token_id=4, num_return_sequences=10000, return_tensors=True)['generated_token_ids']\n",
        "\n",
        "for sent in sents:\n",
        "    temp = []\n",
        "    for token_id in sent[1:]:\n",
        "        if token_id == 4:\n",
        "            break\n",
        "        temp.append(tokenizer.decode(token_id))\n",
        "    sentences.append(temp)\n",
        "    \n",
        "with open(f\"{path}/Evaluation/FID/GPT-2_Small_InferSent.csv\", \"w\", encoding='utf8', newline=\"\") as output_file:\n",
        "    writer = csv.writer(output_file)\n",
        "    writer.writerows(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9QHWGL8TFYi"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "generator_truncated = pipeline(\"text-generation\", model=News_GPT2, tokenizer=tokenizer, return_full_text=False)\n",
        "sents = generator_truncated(\"yesterday\", max_length=max_seq_length, pad_token_id=4, num_return_sequences=1, return_tensors=True)['generated_token_ids']\n",
        "\n",
        "for sent in sents:\n",
        "    temp = []\n",
        "    for token_id in sent[1:]:\n",
        "        if token_id == 4:\n",
        "            break\n",
        "        temp.append(tokenizer.decode(token_id))\n",
        "    sentences.append(temp)\n",
        "    \n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa2KF4duTFYi"
      },
      "source": [
        "Generate Sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qax0ymplTFYi"
      },
      "outputs": [],
      "source": [
        "evaluation.generate_sentences(model=News_GPT2, \n",
        "                              index_decoder=tokenizer.decode, \n",
        "                              print_sentences=True, \n",
        "                              tokenizer=tokenizer, \n",
        "                              model_name=\"News_GPT2\", \n",
        "                              num_sent=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPFNdvJQTFYi"
      },
      "source": [
        "Average sentence length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P93ptTmlTFYj"
      },
      "outputs": [],
      "source": [
        "evaluation.generate_sentences(model=News_GPT2, \n",
        "                              index_decoder=tokenizer.decode, \n",
        "                              print_sentences=False, \n",
        "                              tokenizer=tokenizer, \n",
        "                              model_name=\"News_GPT2\", \n",
        "                              num_sent=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtx4Sz5XTFYj"
      },
      "source": [
        "Prepare the reference data used for Bleu, Self-Bleu and Word Frequency calculations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_31ZNksxTFYj"
      },
      "outputs": [],
      "source": [
        "reference_data = []\n",
        "\n",
        "for sent in validation_data[\"input_ids\"]:\n",
        "    temp = []\n",
        "    for token_id in sent[1:]:\n",
        "        if token_id == 4:\n",
        "            break\n",
        "        temp.append(tokenizer.decode(token_id))\n",
        "    reference_data.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D_ur35VTFYj"
      },
      "source": [
        "Prepare the reference data used for Bleu, Self-Bleu and Word Frequency calculations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKNogoCdTFYk"
      },
      "outputs": [],
      "source": [
        "jsd_sents, jsd_words = evaluation.js_distance(model=News_GPT2, \n",
        "                                              index_decoder=tokenizer.decode, \n",
        "                                              reference_data=reference_data, \n",
        "                                              tokenizer=tokenizer, \n",
        "                                              model_name=\"News_GPT2\", \n",
        "                                              max_seq_length=max_seq_length)\n",
        "\n",
        "print(f\"Jensen-Shannon distance for the sentence length frequencies: {jsd_sents}\")\n",
        "print(f\"Jensen-Shannon distance for the word counts: {jsd_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThdzmO9hTFYk"
      },
      "source": [
        "Calculate Bleu-4 Score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SvLa5zpTFYk"
      },
      "outputs": [],
      "source": [
        "evaluation.bleu_score(model=News_GPT2, \n",
        "                      index_decoder=tokenizer.decode, \n",
        "                      reference_data=reference_data, \n",
        "                      tokenizer=tokenizer, \n",
        "                      model_name=\"News_GPT2\", \n",
        "                      num_sent=10000, \n",
        "                      n_grams=4, \n",
        "                      max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjya7kpwTFYk"
      },
      "source": [
        "Calculate Self-Bleu-4 Score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4OwXaHMTFYl"
      },
      "outputs": [],
      "source": [
        "evaluation.self_bleu_score(model=News_GPT2, \n",
        "                           index_decoder=tokenizer.decode, \n",
        "                           tokenizer=tokenizer, \n",
        "                           model_name=\"News_GPT2\", \n",
        "                           num_sent=10000, \n",
        "                           n_grams=4, \n",
        "                           max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaPKJ40bTFYl"
      },
      "source": [
        "Count Word Frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fUL1N7KTFYl"
      },
      "outputs": [],
      "source": [
        "top_k = 12\n",
        "ref_freq, gen_freq = evaluation.word_freq(model=News_GPT2, \n",
        "                                          index_decoder=tokenizer.decode, \n",
        "                                          reference_data=reference_data, \n",
        "                                          tokenizer=tokenizer, \n",
        "                                          model_name=\"News_GPT2\", \n",
        "                                          max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewuPlzVzTFYl"
      },
      "outputs": [],
      "source": [
        "list(ref_freq.items())[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q50ZpGoTFYm"
      },
      "outputs": [],
      "source": [
        "list(gen_freq.items())[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he1BcF5ETFYl"
      },
      "source": [
        "Word frequency plot:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"YourPathHere\"\n",
        "evaluation.word_freq_plots(reference_freq_dict=ref_freq, \n",
        "                           generated_freq_dict=gen_freq, \n",
        "                           top_k=top_k,\n",
        "                           save_plots=False, \n",
        "                           save_path=save_path)"
      ],
      "metadata": {
        "id": "gX02MhbSX8p2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}