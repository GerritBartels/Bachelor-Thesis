{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GSGAN**"
      ],
      "metadata": {
        "id": "EHaLZARHT5dS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq-XSN1ic_Nk"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bGbnnolMeS5A"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm \n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Adapt this variable to the path of the cloned repository\n",
        "path = \"YourPathHere\"\n",
        "\n",
        "# For importing custom modules\n",
        "import sys\n",
        "sys.path.append(f'{path}/Modules')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k_uz7db0EW_"
      },
      "source": [
        "Mount google drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOmHgJzyI9cQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameters**"
      ],
      "metadata": {
        "id": "oM9IVZrKJ55g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 256 \n",
        "HIDDEN_SIZE = 1024 \n",
        "LEARNING_RATE_PRE = 0.001\n",
        "LEARNING_RATE_POST = 0.00001\n",
        "NUM_EPOCHS_PRE = 15\n",
        "NUM_EPOCHS_POST = 20\n",
        "BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "DNKRUufZKAPd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9H_EpgSrNzx"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import dataset_cleanup"
      ],
      "metadata": {
        "id": "OPxsHcR9lLZp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = f\"{path}/Dataset/news_data_preprocessed.csv\"\n",
        "min_sent_len = 10\n",
        "max_sent_len = 28\n",
        "\n",
        "cleaned_data, max_seq_length = dataset_cleanup(data_path=data_path, \n",
        "                                               min_sent_len=min_sent_len, \n",
        "                                               max_sent_len=max_sent_len)"
      ],
      "metadata": {
        "id": "pjJ9chOzeA3j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B-j3n3ZhpWl2"
      },
      "outputs": [],
      "source": [
        "# Create data for training, consisting of (input,target) pairs\n",
        "train_data = []\n",
        "for sent in cleaned_data:\n",
        "    train_data.append((sent[:-1],sent[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWQ5WDIrf8d"
      },
      "source": [
        "### **Train word2vec embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2jYEg8J0iNb"
      },
      "source": [
        "We use gensim's word2vec function that trains a skip-gram model (with negative sampling) for 50 epochs to create 256 dimensional word embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2OYup8ZYBcB"
      },
      "outputs": [],
      "source": [
        "word2vec_model = Word2Vec(sentences=cleaned_data, size=EMBEDDING_SIZE, window=5, min_count=1, workers=24, sg=1, negative=50, iter=50)\n",
        "# Save the trained embeddings\n",
        "word2vec_model.save(f\"{path}/Skip-Gram Embeddings/skip-gram_embeddings.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcChWYjyui9I"
      },
      "outputs": [],
      "source": [
        "# Load previously saved embeddings\n",
        "word2vec_model = Word2Vec.load(f\"{path}/Skip-Gram Embeddings/skip-gram_embeddings.model\")\n",
        "\n",
        "print(\"Examine the trained embeddings: \")\n",
        "word2vec_model.most_similar(\"<NUM>\", topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9WYEwEn4K3_"
      },
      "source": [
        "Import custom function that converts the word2vec model word vectors into a numpy matrix that is suitable for insertion into our TensorFlow/Keras embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import word2vec_to_matrix"
      ],
      "metadata": {
        "id": "SxnWOxxRoVB0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix, vocab_size = word2vec_to_matrix(word2vec_model=word2vec_model, embedding_size=EMBEDDING_SIZE)"
      ],
      "metadata": {
        "id": "kr673_GYoay3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSeo1hiw7exG"
      },
      "source": [
        "Create a word2index dict in order to convert each token in our train_data dataset to its respective index in the embedding matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_POdYnI2FSQ"
      },
      "outputs": [],
      "source": [
        "word2index_dict = {token: token_index for token_index, token in enumerate(word2vec_model.wv.index2word)}\n",
        "\n",
        "sent2index_input = []\n",
        "sent2index_target = []\n",
        "\n",
        "for input, target in train_data:\n",
        "    input = [word2index_dict[key] for key in input]\n",
        "    target = [word2index_dict[key] for key in target]\n",
        "    sent2index_input.append(input)\n",
        "    sent2index_target.append(target)\n",
        "    \n",
        "# Take a look at one input, target pair\n",
        "print(\"Input sentence: \")\n",
        "print(sent2index_input[0])\n",
        "print(\" \".join([word2vec_model.wv.index2word[i] for i in sent2index_input[0]]))\n",
        "print()\n",
        "print(\"Target sentence: \")\n",
        "print(sent2index_target[0])\n",
        "print(\" \".join([word2vec_model.wv.index2word[i] for i in sent2index_target[0]]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsxk2T0g60Dw"
      },
      "source": [
        "Extract the indices of the **sos** and **eos** tokens for the inference mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ8oFT0xI9cc"
      },
      "outputs": [],
      "source": [
        "start_token = word2index_dict[\"<Start>\"]\n",
        "end_token = word2index_dict[\"<End>\"]\n",
        "print(f\"<Start>: {start_token}\")\n",
        "print(f\"<End>: {end_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3afipGVpARj8"
      },
      "source": [
        "### **Data Pipeline**\n",
        "Creating tf.Dataset objects that are then cached, shuffled, batched and prefetched for efficient training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6Kx9DFzr9Ul"
      },
      "source": [
        "Train data is of form (input, target), where:\n",
        "\n",
        "\n",
        "*   **Input** contains the sentences that will be fed into our GSGAN (serving also as teacher forcing input).\n",
        "\n",
        "*   **Target** contains the sentences that will be used to calculate the loss of our GSGAN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "53aU8BF4qlhY"
      },
      "outputs": [],
      "source": [
        "# We split the data into train data (85%) and test data (15%)\n",
        "train_dataset_input = tf.data.Dataset.from_tensor_slices(sent2index_input[:int(len(sent2index_input)*0.85)])\n",
        "train_dataset_target = tf.data.Dataset.from_tensor_slices(sent2index_target[:int(len(sent2index_target)*0.85)])\n",
        "\n",
        "pre_train_dataset = tf.data.Dataset.zip((train_dataset_input, train_dataset_target)).cache().shuffle(buffer_size=50000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset_target.cache().shuffle(buffer_size=50000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Repeat for test data\n",
        "test_dataset_input = tf.data.Dataset.from_tensor_slices(sent2index_input[int(len(sent2index_input)*0.85):-1])\n",
        "test_dataset_target = tf.data.Dataset.from_tensor_slices(sent2index_target[int(len(sent2index_target)*0.85):-1])\n",
        "\n",
        "pre_test_dataset = tf.data.Dataset.zip((test_dataset_input, test_dataset_target)).cache().shuffle(buffer_size=50000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset_target.cache().shuffle(buffer_size=50000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-Training Generator**\n"
      ],
      "metadata": {
        "id": "1IxhAvHstbR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and pre-train the generator:"
      ],
      "metadata": {
        "id": "W5YJ5j7Otjej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gsgan import Discriminator, Generator, pre_fit, train_GAN"
      ],
      "metadata": {
        "id": "hCPeen1PmTfu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pcww0fVHmqX4"
      },
      "outputs": [],
      "source": [
        "GumbelGAN_Generator = Generator(vocab_size=vocab_size, \n",
        "                                embedding_matrix=embedding_matrix, \n",
        "                                embedding_size=EMBEDDING_SIZE, \n",
        "                                hidden_size=HIDDEN_SIZE, \n",
        "                                tau=5.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DA2CGqNmqX5"
      },
      "outputs": [],
      "source": [
        "save_path = \"YourPathHere\"\n",
        "save_every = 1 # Number of epochs before saving model weights and plots\n",
        "\n",
        "pre_fit(generator=GumbelGAN_Generator,\n",
        "        word2vec_model=word2vec_model, \n",
        "        start_token=start_token, \n",
        "        end_token=end_token, \n",
        "        max_seq_length=max_seq_length,\n",
        "        save_every=save_every,\n",
        "        save_path=save_path,\n",
        "        pre_train_dataset=pre_train_dataset, \n",
        "        pre_test_dataset=pre_test_dataset,\n",
        "        batch_size=BATCH_SIZE, \n",
        "        num_epochs=NUM_EPOCHS_PRE,\n",
        "        learning_rate=LEARNING_RATE_PRE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training GSGAN**"
      ],
      "metadata": {
        "id": "PHGDdLooqkbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create discriminator and load pre-trained generator:"
      ],
      "metadata": {
        "id": "5bYTMgAX1PKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S_DRstXWi9Bn"
      },
      "outputs": [],
      "source": [
        "GumbelGAN_Discriminator = Discriminator(embedding_size=EMBEDDING_SIZE, \n",
        "                                        hidden_size=HIDDEN_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pevpHbHwmt-a"
      },
      "outputs": [],
      "source": [
        "GumbelGAN_Generator = Generator(vocab_size=vocab_size, \n",
        "                                embedding_matrix=embedding_matrix, \n",
        "                                embedding_size=EMBEDDING_SIZE, \n",
        "                                hidden_size=HIDDEN_SIZE, \n",
        "                                tau=5.0)\n",
        "\n",
        "GumbelGAN_Generator.inference_mode(start_token=start_token, \n",
        "                                   end_token=end_token, \n",
        "                                   max_seq_length=max_seq_length)\n",
        "\n",
        "GumbelGAN_Generator.load_weights(f\"{path}/Model Weights/Thesis_Model_Weights/GSGAN_pre\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model:"
      ],
      "metadata": {
        "id": "GIkK2V4E1QMo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHNd0K7nI6fM"
      },
      "outputs": [],
      "source": [
        "save_path = \"YourPathHere\"\n",
        "save_every = 10 # Number of epochs before saving model weights and plots\n",
        "\n",
        "train_GAN(generator=GumbelGAN_Generator, \n",
        "          discriminator=GumbelGAN_Discriminator, \n",
        "          word2vec_model=word2vec_model,\n",
        "          start_token=start_token, \n",
        "          end_token=end_token, \n",
        "          max_seq_length=max_seq_length, \n",
        "          vocab_size=vocab_size,\n",
        "          save_every=save_every, \n",
        "          save_path=save_path,\n",
        "          train_dataset_GAN=train_dataset, \n",
        "          num_epochs=NUM_EPOCHS_POST,\n",
        "          learning_rate=LEARNING_RATE_POST)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "KeOJwKcOqglm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import evaluation module:"
      ],
      "metadata": {
        "id": "vk_9hs6VclZC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RBc-0h9EpUQh"
      },
      "outputs": [],
      "source": [
        "import evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the trained model:"
      ],
      "metadata": {
        "id": "CNwz5xlaqj81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Tpv7CgZcjGp"
      },
      "outputs": [],
      "source": [
        "# Load Generator weights\n",
        "GumbelGAN_Generator = Generator(vocab_size=vocab_size, \n",
        "                                embedding_matrix=embedding_matrix, \n",
        "                                embedding_size=EMBEDDING_SIZE, \n",
        "                                hidden_size=HIDDEN_SIZE, \n",
        "                                tau=0.001)\n",
        "GumbelGAN_Generator.compile()\n",
        "\n",
        "# Feed input through the network to ensure correct loading of the weights\n",
        "GumbelGAN_Generator.inference_mode(start_token=start_token, \n",
        "                                   end_token=end_token, \n",
        "                                   max_seq_length=max_seq_length)\n",
        "\n",
        "GumbelGAN_Generator.load_weights(f\"{path}/Model Weights/Thesis_Model_Weights/GSGAN_post\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GumbelGAN_Discriminator = Discriminator(embedding_size=EMBEDDING_SIZE, \n",
        "                                        hidden_size=HIDDEN_SIZE)\n",
        "\n",
        "for target in train_dataset.take(1):\n",
        "    train_data = tf.one_hot(target, vocab_size)\n",
        "    GumbelGAN_Discriminator(train_data)"
      ],
      "metadata": {
        "id": "RaODzRGKNe4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Summaries:"
      ],
      "metadata": {
        "id": "hg6kyTvrYdEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GumbelGAN_Generator.summary()"
      ],
      "metadata": {
        "id": "Qpwdjr23IAGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GumbelGAN_Discriminator.summary()"
      ],
      "metadata": {
        "id": "6n6i9whlPYGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create csv containing sentences for InferSent: "
      ],
      "metadata": {
        "id": "cmxsr6FfygX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for _ in tqdm(range(10000)):\n",
        "    sentences.append([word2vec_model.wv.index2word[i.numpy()[0]] for i in GumbelGAN_Generator.inference_mode(start_token=start_token, \n",
        "                                                                                                             end_token=end_token, \n",
        "                                                                                                             max_seq_length=max_seq_length-1)])\n",
        "\n",
        "with open(f\"{path}/Evaluation/FID/News_GSGAN_InferSent.csv\", \"w\", encoding='utf8', newline=\"\") as output_file:\n",
        "    writer = csv.writer(output_file)\n",
        "    writer.writerows(sentences)"
      ],
      "metadata": {
        "id": "1QhPpRIdd0NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Sentences:"
      ],
      "metadata": {
        "id": "ecPJV5S8qpwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GumbelGAN_Generator.tau = 0.00001"
      ],
      "metadata": {
        "id": "OTOgsF8iq6pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.generate_sentences(model=GumbelGAN_Generator, \n",
        "                              index_decoder=word2vec_model.wv.index2word, \n",
        "                              print_sentences=True,\n",
        "                              model_name=\"News_GSGAN\",\n",
        "                              latent_sample_gen=None, num_sent=10,\n",
        "                              start_token=start_token, \n",
        "                              end_token=end_token, \n",
        "                              max_seq_length=30)"
      ],
      "metadata": {
        "id": "ftRYMnchsnTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Sentence Length:"
      ],
      "metadata": {
        "id": "FK3wQuM1KwUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.generate_sentences(model=GumbelGAN_Generator, \n",
        "                              index_decoder=word2vec_model.wv.index2word, \n",
        "                              print_sentences=False, \n",
        "                              model_name=\"News_GSGAN\", \n",
        "                              latent_sample_gen=None, \n",
        "                              num_sent=10000,\n",
        "                              start_token=start_token,\n",
        "                              end_token=end_token, \n",
        "                              max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "sLKd0x7tKuS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the reference data used for Bleu, Self-Bleu and Word Frequency calculations:"
      ],
      "metadata": {
        "id": "IX5-kGgOulnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_data = []\n",
        "for sent in sent2index_target[int(len(sent2index_target)*0.85):int(len(sent2index_target)*0.85)+10000]:\n",
        "    temp = []\n",
        "    for token_id in sent:\n",
        "        if token_id == end_token:\n",
        "            break\n",
        "        temp.append(word2vec_model.wv.index2word[token_id])\n",
        "    reference_data.append(temp)"
      ],
      "metadata": {
        "id": "N-LeZRzhu0bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate JS Distance for sentence length frequencies and word counts:"
      ],
      "metadata": {
        "id": "hf73_WytaA8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsd_sents, jsd_words = evaluation.js_distance(model=GumbelGAN_Generator, \n",
        "                                              index_decoder=word2vec_model.wv.index2word, \n",
        "                                              reference_data=reference_data,\n",
        "                                              model_name=\"News_GSGAN\", \n",
        "                                              latent_sample_gen=None, \n",
        "                                              start_token=start_token, \n",
        "                                              end_token=end_token, \n",
        "                                              max_seq_length=max_seq_length)\n",
        "\n",
        "print(f\"Jensen-Shannon distance for the sentence length frequencies: {jsd_sents}\")\n",
        "print(f\"Jensen-Shannon distance for the word counts: {jsd_words}\")"
      ],
      "metadata": {
        "id": "cYpmKSCHZ-Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Bleu-4 Score:"
      ],
      "metadata": {
        "id": "snlBKLJytG3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.bleu_score(model=GumbelGAN_Generator,\n",
        "                      index_decoder=word2vec_model.wv.index2word,\n",
        "                      reference_data=reference_data,\n",
        "                      model_name=\"News_GSGAN\",\n",
        "                      latent_sample_gen=None, \n",
        "                      num_sent=10000, \n",
        "                      n_grams=4, \n",
        "                      start_token=start_token,\n",
        "                      end_token=end_token,\n",
        "                      max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "L0YXG_TEtGRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Self-Bleu-4 Score:"
      ],
      "metadata": {
        "id": "Elgzb12xtLbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.self_bleu_score(model=GumbelGAN_Generator,\n",
        "                           index_decoder=word2vec_model.wv.index2word,\n",
        "                           model_name=\"News_GSGAN\",\n",
        "                           latent_sample_gen=None,\n",
        "                           num_sent=10000,\n",
        "                           n_grams=4,\n",
        "                           start_token=start_token,\n",
        "                           end_token=end_token, \n",
        "                           max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "TZMDusShtPSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count Word Frequency:"
      ],
      "metadata": {
        "id": "SeD4zCmctP5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 12\n",
        "ref_freq, gen_freq = evaluation.word_freq(model=GumbelGAN_Generator,\n",
        "                                          index_decoder=word2vec_model.wv.index2word,\n",
        "                                          reference_data=reference_data,\n",
        "                                          model_name=\"News_GSGAN\", \n",
        "                                          latent_sample_gen=None, \n",
        "                                          start_token=start_token,\n",
        "                                          end_token=end_token,\n",
        "                                          max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "Y0nMNpbotT1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(ref_freq.items())[:top_k]"
      ],
      "metadata": {
        "id": "HUfuj3XmvbF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(gen_freq.items())[:top_k]"
      ],
      "metadata": {
        "id": "bhE1E8dkw6R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word frequency plot:"
      ],
      "metadata": {
        "id": "FQ0VIw2OKhMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"YourPathHere\"\n",
        "evaluation.word_freq_plots(reference_freq_dict=ref_freq, \n",
        "                           generated_freq_dict=gen_freq, \n",
        "                           top_k=top_k,\n",
        "                           save_plots=False, \n",
        "                           save_path=save_path)"
      ],
      "metadata": {
        "id": "sc5vtR1lT8LC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}