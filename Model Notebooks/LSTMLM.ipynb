{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7BfRmCmzsx1"
      },
      "source": [
        "# **LSTM Language Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a2ZjwhcrKC5"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoJg2HIqIX5L"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm \n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Adapt this variable to the path of the cloned repository\n",
        "path = \"YourPathHere\"\n",
        "\n",
        "# For importing custom modules\n",
        "import sys\n",
        "sys.path.append(f'{path}/Modules')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H94x7sr0FY2"
      },
      "source": [
        "Mount Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOmHgJzyI9cQ",
        "outputId": "09f53e45-f4c6-4f67-b7a9-12dfd4c9b5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameters**"
      ],
      "metadata": {
        "id": "oM9IVZrKJ55g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 256 \n",
        "HIDDEN_SIZE = 1024\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "DNKRUufZKAPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn-V5h2A0LIy"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import dataset_cleanup"
      ],
      "metadata": {
        "id": "JbvPH_l4lZB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = f\"{path}/Dataset/news_data_preprocessed.csv\"\n",
        "min_sent_len = 10\n",
        "max_sent_len = 28\n",
        "\n",
        "cleaned_data, max_seq_length = dataset_cleanup(data_path=data_path, \n",
        "                                               min_sent_len=min_sent_len, \n",
        "                                               max_sent_len=max_sent_len)"
      ],
      "metadata": {
        "id": "O_k3vNslkt4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMXS25_wodo0"
      },
      "outputs": [],
      "source": [
        "# Create data for training, consisting of (input,target) pairs\n",
        "train_data = []\n",
        "for sent in cleaned_data:\n",
        "    train_data.append((sent[:-1],sent[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWQ5WDIrf8d"
      },
      "source": [
        "### **Train Word2Vec embeddings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2jYEg8J0iNb"
      },
      "source": [
        "We use gensim's word2vec function that trains a skip-gram model (with negative sampling) for 50 epochs to create 256 dimensional word embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2OYup8ZYBcB"
      },
      "outputs": [],
      "source": [
        "word2vec_model = Word2Vec(sentences=cleaned_data, size=EMBEDDING_SIZE, window=5, min_count=1, workers=24, sg=1, negative=50, iter=50)\n",
        "# Save the trained embeddings\n",
        "word2vec_model.save(f\"{path}/Skip-Gram Embeddings/skip-gram_embeddings.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcChWYjyui9I"
      },
      "outputs": [],
      "source": [
        "# Load the previously saved embeddings\n",
        "word2vec_model = Word2Vec.load(f\"{path}/Skip-Gram Embeddings/skip-gram_embeddings.model\")\n",
        "\n",
        "print(\"Examine the trained embeddings: \")\n",
        "word2vec_model.most_similar(\"<NUM>\", topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9WYEwEn4K3_"
      },
      "source": [
        "Import custom function that converts the word2vec model word vectors into a numpy matrix that is suitable for insertion into our TensorFlow/Keras embedding layer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import word2vec_to_matrix"
      ],
      "metadata": {
        "id": "SxnWOxxRoVB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix, vocab_size = word2vec_to_matrix(word2vec_model=word2vec_model, embedding_size=EMBEDDING_SIZE)"
      ],
      "metadata": {
        "id": "kr673_GYoay3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSeo1hiw7exG"
      },
      "source": [
        "Create a word2index dict in order to convert each token in our train_data dataset to its respective index in the embedding matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_POdYnI2FSQ"
      },
      "outputs": [],
      "source": [
        "word2index_dict = {token: token_index for token_index, token in enumerate(word2vec_model.wv.index2word)}\n",
        "\n",
        "sent2index_input = []\n",
        "sent2index_target = []\n",
        "\n",
        "for input, target in train_data:\n",
        "    input = [word2index_dict[key] for key in input]\n",
        "    target = [word2index_dict[key] for key in target]\n",
        "    sent2index_input.append(input)\n",
        "    sent2index_target.append(target)\n",
        "\n",
        "# Take a look at one input, target pair\n",
        "print(\"Input sentence: \")\n",
        "print(sent2index_input[0])\n",
        "print(\" \".join([word2vec_model.wv.index2word[i] for i in sent2index_input[0]]))\n",
        "print()\n",
        "print(\"Target sentence: \")\n",
        "print(sent2index_target[0])\n",
        "print(\" \".join([word2vec_model.wv.index2word[i] for i in sent2index_target[0]]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07VBpwC87Wuo"
      },
      "source": [
        "Extract the indices of the **sos** and **eos** tokens for the inference mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ8oFT0xI9cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd6f659-c8eb-4643-f667-3ef9b2ed491b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Start>: 0\n",
            "<End>: 2\n"
          ]
        }
      ],
      "source": [
        "start_token = word2index_dict[\"<Start>\"]\n",
        "end_token = word2index_dict[\"<End>\"]\n",
        "print(f\"<Start>: {start_token}\")\n",
        "print(f\"<End>: {end_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3afipGVpARj8"
      },
      "source": [
        "### **Data Pipeline**\n",
        "Creating tf.Dataset objects that are then cached, shuffled, batched and prefetched for efficient training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL0xt1Wx3EkZ"
      },
      "source": [
        "Train data is of form (input, target), where:\n",
        "\n",
        "\n",
        "*   **Input** contains the sentences that will be fed into our LSTMLM (serving also as teacher forcing input).\n",
        "\n",
        "*   **Target** contains the sentences that will be used to calculate the loss of our LSTMLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol0exgCdpWFO"
      },
      "outputs": [],
      "source": [
        "# We split the data into train data (85%) and test data (15%)\n",
        "train_dataset_input = tf.data.Dataset.from_tensor_slices(sent2index_input[:int(len(sent2index_input)*0.85)])\n",
        "train_dataset_target = tf.data.Dataset.from_tensor_slices(sent2index_target[:int(len(sent2index_target)*0.85)])\n",
        "\n",
        "train_dataset = tf.data.Dataset.zip((train_dataset_input, train_dataset_target)).cache().shuffle(buffer_size=50000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Repeat for test data\n",
        "test_dataset_input = tf.data.Dataset.from_tensor_slices(sent2index_input[int(len(sent2index_input)*0.85):-1])\n",
        "test_dataset_target = tf.data.Dataset.from_tensor_slices(sent2index_target[int(len(sent2index_target)*0.85):-1])\n",
        "\n",
        "test_dataset = tf.data.Dataset.zip((test_dataset_input, test_dataset_target)).cache().shuffle(buffer_size=50000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VSVX8DoeBs4"
      },
      "source": [
        "## **Training LSTMLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfgZzX_ZdNMY"
      },
      "source": [
        "Create and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lstmlm import LSTMLM, trainModel"
      ],
      "metadata": {
        "id": "v3-DK9uDEDHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "News_LSTMLM = LSTMLM(vocab_size=vocab_size, \n",
        "                     embedding_matrix=embedding_matrix,\n",
        "                     embedding_size=EMBEDDING_SIZE, \n",
        "                     hidden_size=HIDDEN_SIZE)"
      ],
      "metadata": {
        "id": "CFxr45tPEe8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1T6cyKXEe8k"
      },
      "outputs": [],
      "source": [
        "save_path = \"YourPathHere\"\n",
        "save_every = 10 # Number of epochs before saving model weights and plots \n",
        "\n",
        "trainModel(model=News_LSTMLM, \n",
        "           word2vec_model=word2vec_model,\n",
        "           start_token=start_token,\n",
        "           end_token=end_token, \n",
        "           max_seq_length=max_seq_length,\n",
        "           save_every=save_every,\n",
        "           save_path=save_path, \n",
        "           train_dataset=train_dataset, \n",
        "           test_dataset=test_dataset, \n",
        "           loss_function=tf.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "           num_epochs=1,\n",
        "           learning_rate=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeOJwKcOqglm"
      },
      "source": [
        "## **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_9hs6VclZC"
      },
      "source": [
        "Import evaluation module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBc-0h9EpUQh"
      },
      "outputs": [],
      "source": [
        "import evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNwz5xlaqj81"
      },
      "source": [
        "Load the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5akm0IjdsJg"
      },
      "outputs": [],
      "source": [
        "News_LSTMLM = LSTMLM(vocab_size=vocab_size, \n",
        "                     embedding_matrix=embedding_matrix, \n",
        "                     embedding_size=EMBEDDING_SIZE, \n",
        "                     hidden_size=HIDDEN_SIZE)\n",
        "News_LSTMLM.compile()\n",
        "\n",
        "# Feed input through the network to ensure correct loading of the weights\n",
        "News_LSTMLM.inference_mode(start_token=start_token, \n",
        "                           end_token=end_token, \n",
        "                           max_seq_length=max_seq_length)\n",
        "\n",
        "News_LSTMLM.load_weights(f\"{path}Model Weights/Thesis_Model_Weights/LSTMLM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Summary:"
      ],
      "metadata": {
        "id": "016bWudiE-0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "News_LSTMLM.summary()"
      ],
      "metadata": {
        "id": "vHEdwGk9C56O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmxsr6FfygX6"
      },
      "source": [
        "Create csv containing sentences for InferSent: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QhPpRIdd0NO"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for _ in tqdm(range(10000)):\n",
        "    sentences.append([word2vec_model.wv.index2word[i.numpy()[0]] for i in News_LSTMLM.inference_mode(start_token=start_token, \n",
        "                                                                                                     end_token=end_token, \n",
        "                                                                                                     max_seq_length=max_seq_length-1, \n",
        "                                                                                                     states=None)])\n",
        "\n",
        "with open(f\"{path}/Evaluation/FID/News_LSTMLM_InferSent.csv\", \"w\", encoding='utf8', newline=\"\") as output_file:\n",
        "    writer = csv.writer(output_file)\n",
        "    writer.writerows(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPJV5S8qpwN"
      },
      "source": [
        "Generate Sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftRYMnchsnTN"
      },
      "outputs": [],
      "source": [
        "evaluation.generate_sentences(model=News_LSTMLM, \n",
        "                              index_decoder=word2vec_model.wv.index2word, \n",
        "                              print_sentences=True, \n",
        "                              model_name=\"News_LSTMLM\", \n",
        "                              latent_sample_gen=None, \n",
        "                              num_sent=10, \n",
        "                              start_token=start_token, \n",
        "                              end_token=end_token, \n",
        "                              max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksrtqlf4JT1M"
      },
      "source": [
        "Average Sentence Length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HqsdpyrJXHv"
      },
      "outputs": [],
      "source": [
        "evaluation.generate_sentences(model=News_LSTMLM,\n",
        "                              index_decoder=word2vec_model.wv.index2word, \n",
        "                              print_sentences=False, \n",
        "                              model_name=\"News_LSTMLM\", \n",
        "                              latent_sample_gen=None, \n",
        "                              num_sent=10000, \n",
        "                              start_token=start_token,\n",
        "                              end_token=end_token, \n",
        "                              max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX5-kGgOulnr"
      },
      "source": [
        "Prepare the reference data used for Bleu, Self-Bleu and Word Frequency calculations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-LeZRzhu0bj"
      },
      "outputs": [],
      "source": [
        "reference_data = []\n",
        "for sent in sent2index_target[int(len(sent2index_target)*0.85):int(len(sent2index_target)*0.85)+10000]:\n",
        "    temp = []\n",
        "    for token_id in sent:\n",
        "        if token_id == end_token:\n",
        "            break\n",
        "        temp.append(word2vec_model.wv.index2word[token_id])\n",
        "    reference_data.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate JS Distance for sentence length frequencies and word counts:"
      ],
      "metadata": {
        "id": "uE0kTvfMwSeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsd_sents, jsd_words = evaluation.js_distance(model=News_LSTMLM, \n",
        "                                              index_decoder=word2vec_model.wv.index2word,\n",
        "                                              reference_data=reference_data, \n",
        "                                              model_name=\"News_LSTMLM\",\n",
        "                                              latent_sample_gen=None,\n",
        "                                              start_token=start_token,\n",
        "                                              end_token=end_token, \n",
        "                                              max_seq_length=max_seq_length)\n",
        "\n",
        "print(f\"Jensen-Shannon distance for the sentence length frequencies: {jsd_sents}\")\n",
        "print(f\"Jensen-Shannon distance for the word counts: {jsd_words}\")"
      ],
      "metadata": {
        "id": "VWGlJcT-wRwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlBKLJytG3b"
      },
      "source": [
        "Calculate Bleu-4 Score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0YXG_TEtGRj"
      },
      "outputs": [],
      "source": [
        "evaluation.bleu_score(model=News_LSTMLM,\n",
        "                      index_decoder=word2vec_model.wv.index2word,\n",
        "                      reference_data=reference_data,\n",
        "                      model_name=\"News_LSTMLM\",\n",
        "                      latent_sample_gen=None, \n",
        "                      num_sent=10000,\n",
        "                      n_grams=4,\n",
        "                      start_token=start_token,\n",
        "                      end_token=end_token,\n",
        "                      max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elgzb12xtLbl"
      },
      "source": [
        "Calculate Self-Bleu-4 Score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZMDusShtPSO"
      },
      "outputs": [],
      "source": [
        "evaluation.self_bleu_score(model=News_LSTMLM, \n",
        "                           index_decoder=word2vec_model.wv.index2word, \n",
        "                           model_name=\"News_LSTMLM\", \n",
        "                           latent_sample_gen=None, \n",
        "                           num_sent=10000, \n",
        "                           n_grams=4, \n",
        "                           start_token=start_token, \n",
        "                           end_token=end_token, \n",
        "                           max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeD4zCmctP5e"
      },
      "source": [
        "Count Word Frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0nMNpbotT1V"
      },
      "outputs": [],
      "source": [
        "top_k = 12\n",
        "ref_freq, gen_freq = evaluation.word_freq(model=News_LSTMLM, \n",
        "                                          index_decoder=word2vec_model.wv.index2word,\n",
        "                                          reference_data=reference_data,\n",
        "                                          model_name=\"News_LSTMLM\",\n",
        "                                          latent_sample_gen=None,\n",
        "                                          start_token=start_token,\n",
        "                                          end_token=end_token,\n",
        "                                          max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhE1E8dkw6R-"
      },
      "outputs": [],
      "source": [
        "list(ref_freq.items())[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUfuj3XmvbF3"
      },
      "outputs": [],
      "source": [
        "list(gen_freq.items())[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIzNib5GHG4i"
      },
      "source": [
        "Word frequency plot:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"YourPathHere\"\n",
        "evaluation.word_freq_plots(reference_freq_dict=ref_freq, \n",
        "                           generated_freq_dict=gen_freq, \n",
        "                           top_k=top_k,\n",
        "                           save_plots=False, \n",
        "                           save_path=save_path)"
      ],
      "metadata": {
        "id": "YOSBHH0OOxyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}