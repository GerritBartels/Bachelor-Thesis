# About this Repository

This repository contains the code to our (Jacob Dudek and Gerrit Bartels) bachelor thesis. We implemented 5 Neural Networks and compared their performance on the task of unconditional text generation. We set up a thorough evaluation scheme, using common automatic evaluation methods and supplemented these results with a human evaluation. Furthermore, we propose two evaluation metrics based on the Jennsen-Shannon Distance that help with judging how well the underlying data distribution has been learnt. 

---
# The Dataset

As dataset for training all models we use a monolingual news crawl from the Fifth Conference on Machine Translation (WMT20) that can directly be obtained from the conference website: \href{https://data.statmt.org/news-crawl/en/}{data.statmt.org/news-crawl/en/}. It contains approximately 44M English news sentences extracted from online newspaper articles that were published throughout 2019. After applying our preprocessing steps (see preprocessing notebook) we obtained a dataset comprising approximately 240k sentences with an average length of 18.65 and a vocabulary size of 6801.

---
# The Models

## LSTMLM

## GSGAN

## GPT-2 Small

## cVAELM

## LaTextGAN

---
# Evaluation Methods

---
# Results

---
# Example Sentences
